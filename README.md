# ðŸ§ª Data Poisoning: Backdoor Attack Demo

## ðŸŽ¯ Overview
This project demonstrates a **Backdoor Attack** (a type of Data Poisoning) on a machine learning model.
The core asset is a Jupyter Notebook that trains an email spam classifier but injects a "trojan horse" into the training data.

## ðŸ“‚ Project Structure
The project follows a 7-stage holistic development lifecycle:

-   **[1_Real](1_Real/README.md)**: Objectives - Why we are doing this (Demonstrating security vulnerabilities).
-   **[2_Environment](2_Environment/README.md)**: Tools - Google Colab, Jupyter, Python.
-   **[3_UI](3_UI/README.md)**: Interface - The notebook as the user interface.
-   **[4_Formula](4_Formula/README.md)**: Logic - TF-IDF Vectorization and Logistic Regression.
-   **[5_Symbols](5_Symbols/README.md)**: **Code - The `backdoor_attack_demo.ipynb` lives here.**
-   **[6_Semblance](6_Semblance/README.md)**: Errors - Debugging attacks and potential misclassifications.
-   **[7_Testing](7_Testing/README.md)**: Validation - Verifying the attack success rate.

## ðŸš€ Getting Started
1.  Navigate to `5_Symbols/backdoor_attack_demo.ipynb`.
2.  Open the file in a Jupyter environment (VS Code or Google Colab).
3.  Run the cells to see the attack in action.
